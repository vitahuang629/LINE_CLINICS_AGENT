import requests
from langchain_core.embeddings import Embeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
import pandas as pd
from typing import List
from tqdm import tqdm
import os
import jieba
from langchain.retrievers import BM25Retriever, EnsembleRetriever
import pickle
from langchain.schema import Document
from pydantic import Field
from langchain_openai import OpenAIEmbeddings
from utils.shared_resources import embedding_model, chinese_tokenizer



# #ä¸­æ–‡åˆ†è©
# def chinese_tokenizer(text):
#     return [token for token in jieba.cut(text) if token.strip()]

# # ğŸ”§ ä½ çš„ AWS Ollama embedding ç«¯é»
# OLLAMA_URL = OLLAMA_URL
# EMBED_MODEL = "bge-m3"

# # âœ… è‡ªè¨‚åµŒå…¥æ¨¡å‹é¡åˆ¥
# class OllamaEmbeddings(Embeddings):
#     """ä½¿ç”¨Ollama APIçš„åµŒå…¥æ¨¡å‹"""
    
#     def __init__(self, ollama_url: str = None, model: str = None):
#         self.ollama_url = ollama_url or OLLAMA_URL
#         self.model = model or EMBED_MODEL
    
#     def embed_documents(self, texts: List[str]) -> List[List[float]]:
#         """åµŒå…¥æ–‡æª”åˆ—è¡¨"""
#         return [self.embed_query(text) for text in texts]
    
#     def embed_query(self, text: str) -> List[float]:
#         """åµŒå…¥å–®å€‹æŸ¥è©¢"""
#         try:
#             response = requests.post(
#                 f"{self.ollama_url}/api/embeddings",
#                 json={"model": self.model, "prompt": text},
#                 timeout=60
#             )
#             response.raise_for_status()
#             embedding = response.json().get("embedding", [])
            
#             if not embedding:
#                 print(f"è­¦å‘Š: å¾APIç²å–åˆ°ç©ºå‘é‡ï¼Œä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
#                 return [0.0] * 1024  # æ ¹æ“šæ‚¨çš„æ¨¡å‹èª¿æ•´ç¶­åº¦
                
#             return embedding
#         except Exception as e:
#             print(f"åµŒå…¥æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
#             return [0.0] * 1024  # æ ¹æ“šæ‚¨çš„æ¨¡å‹èª¿æ•´ç¶­åº¦




def get_ensemble_retriever():
    # è®€å–æ•¸æ“š
    doc = pd.read_csv("C:/Users/bexo6/OneDrive/æ¡Œé¢/line_clinics_agent/data/clinics_introductions3.csv", encoding="big5")
    # print(doc)

    documents_for_vector = []
    documents_for_keyword = []


    for idx, row in doc.iterrows():
        # æ­£å¸¸ç‰ˆæœ¬ï¼ˆçµ¦ vector retrieverï¼‰
        normal_content = (
            f"ç™‚ç¨‹åç¨±ï¼š{row['name']}\n"
            f"ä»‹ç´¹å…§å®¹ï¼š{row['introduction']}\n"
            f"é©åˆå°è±¡ï¼š{row['suitable_for']}\n"
            f"å¸¸è¦‹é—œéµå­—ï¼š{row['keywords']}ï¼ˆä¾‹å¦‚ä½¿ç”¨è€…å¯èƒ½æœƒèªªå‡ºé€™äº›è©ï¼‰\n"   
        )
        
        # åŠ é‡ç‰ˆæœ¬ï¼ˆçµ¦ keyword retrieverï¼‰
        suitable_for_weighted = (row['suitable_for'] + " ") * 3
        keyword_content = (
            f"ç™‚ç¨‹åç¨±ï¼š{row['name']}\n"
            f"ä»‹ç´¹å…§å®¹ï¼š{row['introduction']}\n"
            f"é©åˆå°è±¡ï¼š{suitable_for_weighted}\n"
            f"å¸¸è¦‹é—œéµå­—ï¼š{row['keywords']}ï¼ˆä¾‹å¦‚ä½¿ç”¨è€…å¯èƒ½æœƒèªªå‡ºé€™äº›è©ï¼‰\n"
        )
        
        documents_for_vector.append(
            Document(page_content=normal_content, metadata={"clinic": row["name"],
                                                            "category": row["category"],
                                                            "document_id": idx})
        )
        documents_for_keyword.append(
            Document(page_content=keyword_content, metadata={"clinic": row["name"], 
                                                            "category": row["category"],
                                                            "document_id": idx})
        )

    persist_dir = "./chroma_token_split"  # ä½ è¦å­˜æ”¾è³‡æ–™åº«çš„è³‡æ–™å¤¾è·¯å¾‘

    #å¦‚æœé€™æ®µé‡è¤‡åŸ·è¡Œ, æœƒé€ æˆretrieveré‡è¤‡, ç¬¬ä¸€æ¬¡ä½¿ç”¨å°±å¥½
    if not os.path.exists(persist_dir):
        vectorstore = Chroma.from_documents(
            documents=documents_for_vector,
            embedding=embedding_model,
            persist_directory=persist_dir
        )
    else:
        print("âœ… è³‡æ–™åº«å·²å­˜åœ¨ï¼Œç•¥éå»ºç«‹æ­¥é©Ÿ")
        vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embedding_model
        )

    # ä¹‹å¾Œæƒ³ç”¨ retrieverï¼Œå°±å¾æœ¬åœ°è¼‰å…¥
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})


    # å»ºç«‹/è¼‰å…¥ BM25 keyword retrieverï¼ˆä½¿ç”¨å¿«å–ï¼‰
    bm25_path = "./bm25.pkl"

    if os.path.exists(bm25_path):
        print("âœ… è¼‰å…¥å¿«å–çš„ BM25 Retriever")
        with open(bm25_path, "rb") as f:
            keyword_retriever = pickle.load(f)
    else:
        print("ğŸš§ å»ºç«‹ BM25 Retriever ä¸¦å¿«å–")
        keyword_retriever = BM25Retriever.from_documents(documents_for_keyword, preprocess_func=chinese_tokenizer,k = 3)
        with open(bm25_path, "wb") as f:
            pickle.dump(keyword_retriever, f)


    ensemble_retriever = EnsembleRetriever(retrievers = [vector_retriever, keyword_retriever], weights = [0.3, 0.7])
    return ensemble_retriever

# if __name__ == "__main__":
#     retriever = get_ensemble_retriever()  # <== è¦å‘¼å«å‡½å¼æ‹¿ retriever å›ä¾†
#     docs = retriever.get_relevant_documents("æ‰“å‘¼")
#     for i, doc in enumerate(docs):
#         print(f"Doc {i+1}: {doc.page_content}")